{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaZAwKJDKQQD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a2d6cfe1-83a6-4d8a-c09c-6001550a91a8"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "import pandas as pd\n",
        "import pandas.testing as tm\n",
        "import pandas.util.testing as tm\n",
        "import seaborn as sns \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from tqdm import tqdm_notebook\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score\n",
        "import xgboost as xgb\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.svm import SVR\n",
        "import json\n",
        "from tqdm import tqdm_notebook\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFRd_dFrYC2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading the train and test dataset\n",
        "df_1=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Case Study 1/mercedes-benz-greener-manufacturing/train.csv/train.csv')\n",
        "df_2=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Case Study 1/mercedes-benz-greener-manufacturing/test.csv/test.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QViO6_BAb8CD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def final_pred(df_1, df_2):\n",
        "    # converting categorical features to numerical features\n",
        "    x_dummies_1=pd.get_dummies(df_1)\n",
        "    x_dummies_2=pd.get_dummies(df_2)\n",
        "\n",
        "    missing_cols = set( x_dummies_1.columns ) - set( x_dummies_2.columns )\n",
        "    y=x_dummies_1['y']\n",
        "    # align dataframes\n",
        "    x_dummies_1, x_dummies_2= x_dummies_1.align(x_dummies_2, join='inner', axis=1)\n",
        "    \n",
        "    # creating a list of columns which have only zeros\n",
        "    zeros=[]\n",
        "    for i,j in x_dummies_1.any().items():#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.iteritems.html\n",
        "        if j==False:\n",
        "            zeros.append(i)\n",
        "\n",
        "    # dropping the columns containing only zeros\n",
        "    x_dummies_1 = x_dummies_1.drop(zeros, axis=1)\n",
        "    x_dummies_2 = x_dummies_2.drop(zeros, axis=1)\n",
        "    # saving the dataframes as a backup for further computations\n",
        "    x_1_safe = x_dummies_1\n",
        "    x_2_safe = x_dummies_2 \n",
        "    x_dummies_1['y']=y\n",
        "    # Dropping Outliers\n",
        "    x_filtered= x_dummies_1[x_dummies_1['y']>70]#https://www.geeksforgeeks.org/drop-rows-from-the-dataframe-based-on-certain-condition-applied-on-a-column/\n",
        "    x_filtered= x_filtered[x_filtered['y']<150]\n",
        "    y=x_filtered['y']\n",
        "    x_filtered=x_filtered.drop(['y'], axis=1)\n",
        "    top_20_features= ['ID', 'X14', 'X29', 'X54', 'X76', 'X118', 'X119', 'X127', 'X132',\n",
        "       'X136', 'X189', 'X222', 'X232', 'X263', 'X279', 'X311', 'X314', 'X315',\n",
        "       'X6_g', 'X6_j']\n",
        "    def calc_vif(X):\n",
        "\n",
        "        # Calculationg VIF\n",
        "        vif=pd.DataFrame()\n",
        "        vif['variables']=X.columns\n",
        "        vif['VIF']= [variance_inflation_factor(X.values, i) for i in tqdm_notebook(range(X.shape[1]))]\n",
        "        \n",
        "        return (vif)\n",
        "    '''vif_values=calc_vif(x_filtered)\n",
        "    vif_values=vif_values.sort_values(by='VIF', ascending=True)\n",
        "    lst_variables= list(vif_values['variables'])\n",
        "    lst_vif= list(vif_values['VIF'])\n",
        "    vif_dict = dict(zip(lst_variables, lst_vif))'''\n",
        "    '''inf_indices=[]\n",
        "    count=0\n",
        "    for k,v in vif_dict.items():\n",
        "        if np.isinf(v):\n",
        "            inf_indices.append(k)\n",
        "            count+=1'''\n",
        "    '''# removing top_20_features from the features that need to be dropped from VIF\n",
        "    inf_indices=list(set(inf_indices)-set(top_20_features))'''\n",
        "    with open('/content/drive/My Drive/Colab Notebooks/Case Study 1/inf_indices_complete.txt', 'r') as f:\n",
        "        checc=json.loads(f.read())\n",
        "    inf_indices=checc\n",
        "    # dropping the features \n",
        "    x_filtered = x_filtered.drop(inf_indices, axis=1)\n",
        "    x_dummies_2= x_dummies_2.drop(inf_indices, axis=1)\n",
        "    # generating SVD features\n",
        "    tsvd= TruncatedSVD(n_components=2, random_state=42)\n",
        "    tsvd_train= tsvd.fit_transform(x_filtered)\n",
        "    tsvd_test=  tsvd.transform(x_dummies_2)\n",
        "    # generating PCA features\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    pca_train= pca.fit_transform(x_filtered)\n",
        "    pca_test= pca.transform(x_dummies_2)\n",
        "    # generating ICA features\n",
        "    ica=FastICA(n_components=2, random_state=42)\n",
        "    ica_train= ica.fit_transform(x_filtered)\n",
        "    ica_test= ica.transform(x_dummies_2)\n",
        "    # adding these dimensionality reduction features to the dataset\n",
        "    for i in range(0, tsvd_train.shape[1]):\n",
        "        x_filtered['tsvd_'+str(i)]= tsvd_train[:, i] \n",
        "        x_dummies_2['tsvd_'+str(i)]= tsvd_test[:, i]\n",
        "        x_filtered['pca_'+str(i)]= pca_train[:, i]\n",
        "        x_dummies_2['pca_'+str(i)]= pca_test[:, i]\n",
        "        x_filtered['ica_'+str(i)]= ica_train[:, i]\n",
        "        x_dummies_2['ica_'+str(i)]= ica_test[:, i]\n",
        "\n",
        "    # adding new features using feature interaction of important features\n",
        "    x_filtered['X64 + X218']=x_filtered['X64']+x_filtered['X218']\n",
        "    x_dummies_2['X64 + X218']=x_dummies_2['X64']+x_dummies_2['X218']\n",
        "    \n",
        "    x_filtered['X218 + X224 + X273']=x_filtered['X218']+x_filtered['X224'] + x_filtered['X273']\n",
        "    x_dummies_2['X218 + X224 + X273']=x_dummies_2['X218']+x_dummies_2['X224'] + x_dummies_2['X273']\n",
        "    \n",
        "    x_filtered['X64 + X224 + X273']=x_filtered['X64']+x_filtered['X224'] + x_filtered['X273']\n",
        "    x_dummies_2['X64 + X224 + X273']=x_dummies_2['X64']+x_dummies_2['X224'] + x_dummies_2['X273']\n",
        "    \n",
        "    x_filtered['X314 + X315']=x_filtered['X314']+x_filtered['X315']\n",
        "    x_dummies_2['X314 + X315']=x_dummies_2['X314']+x_dummies_2['X315']\n",
        "    \n",
        "    x_filtered['X314 + X315 + X29']=x_filtered['X314']+x_filtered['X315'] + x_filtered['X29']\n",
        "    x_dummies_2['X314 + X315 + X29']=x_dummies_2['X314']+x_dummies_2['X315'] + x_dummies_2['X29']\n",
        "    \n",
        "    x_filtered['X314 + X315 + X118']=x_filtered['X314']+x_filtered['X315'] + x_filtered['X118']\n",
        "    x_dummies_2['X314 + X315 + X118']=x_dummies_2['X314']+x_dummies_2['X315'] + x_dummies_2['X118']\n",
        "    \n",
        "    x_filtered['X127 + X189']=x_filtered['X127']+x_filtered['X189']\n",
        "    x_dummies_2['X127 + X189']=x_dummies_2['X127']+x_dummies_2['X189']\n",
        "    \n",
        "    x_filtered['X118 + X54']=x_filtered['X118']+x_filtered['X54']\n",
        "    x_dummies_2['X118 + X54']=x_dummies_2['X118']+x_dummies_2['X54']\n",
        "    \n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_filtered, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # applying RandomForestRegressor model \n",
        "    # Number of trees in a Random forest\n",
        "    n_estimators=[int(x) for x in np.linspace (start=100, stop=1000, num=10)]\n",
        "    \n",
        "    # Number of features to consider at every split\n",
        "    max_features=['auto', 'sqrt']\n",
        "     \n",
        "    # Maximun no of levels in a tree\n",
        "    max_depth=[int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "    max_depth.append(None)\n",
        "    \n",
        "    # minimum number of samples required to split a node\n",
        "    min_samples_split = np.arange(50,250,20)\n",
        "    \n",
        "    # Minimum number of samples required at each leaf node\n",
        "    min_samples_leaf = np.arange(5,50,5)\n",
        "    # Method of selecting samples for training each tree\n",
        "    bootstrap = [True, False]\n",
        "    \n",
        "    # Create the random grid\n",
        "    random_grid = {'n_estimators': n_estimators,\n",
        "                   'max_features': max_features,\n",
        "                   'max_depth': max_depth,\n",
        "                   'min_samples_split': min_samples_split,\n",
        "                   'min_samples_leaf': min_samples_leaf,\n",
        "                   'bootstrap': bootstrap}\n",
        "    # First create the base model to tune\n",
        "    rf = RandomForestRegressor()\n",
        "    # Random search of parameters, using 3 fold cross validation, \n",
        "    # search across 100 different combinations, and use all available cores\n",
        "    '''rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "    # Fit the random search model \n",
        "    rf_random.fit(x_filtered, y)\n",
        "    best_rf=rf_random.best_estimator_'''\n",
        "    best_rf= RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
        "                          max_depth=70, max_features='auto', max_leaf_nodes=None,\n",
        "                          max_samples=None, min_impurity_decrease=0.0,\n",
        "                          min_impurity_split=None, min_samples_leaf=40,\n",
        "                          min_samples_split=110, min_weight_fraction_leaf=0.0,\n",
        "                          n_estimators=500, n_jobs=None, oob_score=False,\n",
        "                          random_state=None, verbose=0, warm_start=False)\n",
        "    # fitting the best model on to the dataset\n",
        "    best_rf.fit(x_filtered, y)\n",
        "    y_train_pred=best_rf.predict(x_filtered)\n",
        "    y_test_pred=best_rf.predict(x_dummies_2)\n",
        "    return y_train_pred, y_test_pred\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxAfa385b79r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_pred, y_test_pred = final_pred(df_1, df_2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07of1Lj0b72e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "whIae9IYXYzT",
        "colab": {}
      },
      "source": [
        "# loading the train and test dataset\n",
        "df_1=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Case Study 1/mercedes-benz-greener-manufacturing/train.csv/train.csv')\n",
        "df_2=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Case Study 1/mercedes-benz-greener-manufacturing/test.csv/test.csv')\n",
        "def final_metric(df_1, df_2):\n",
        "    # converting categorical features to numerical features\n",
        "    x_dummies_1=pd.get_dummies(df_1)\n",
        "    x_dummies_2=pd.get_dummies(df_2)\n",
        "\n",
        "    missing_cols = set( x_dummies_1.columns ) - set( x_dummies_2.columns )\n",
        "    y=x_dummies_1['y']\n",
        "    # align dataframes\n",
        "    x_dummies_1, x_dummies_2= x_dummies_1.align(x_dummies_2, join='inner', axis=1)\n",
        "    \n",
        "    # creating a list of columns which have only zeros\n",
        "    zeros=[]\n",
        "    for i,j in x_dummies_1.any().items():#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.iteritems.html\n",
        "        if j==False:\n",
        "            zeros.append(i)\n",
        "\n",
        "    # dropping the columns containing only zeros\n",
        "    x_dummies_1 = x_dummies_1.drop(zeros, axis=1)\n",
        "    x_dummies_2 = x_dummies_2.drop(zeros, axis=1)\n",
        "    # saving the dataframes as a backup for further computations\n",
        "    x_1_safe = x_dummies_1\n",
        "    x_2_safe = x_dummies_2 \n",
        "    x_dummies_1['y']=y\n",
        "    # Dropping Outliers\n",
        "    x_filtered= x_dummies_1[x_dummies_1['y']>70]#https://www.geeksforgeeks.org/drop-rows-from-the-dataframe-based-on-certain-condition-applied-on-a-column/\n",
        "    x_filtered= x_filtered[x_filtered['y']<150]\n",
        "    y=x_filtered['y']\n",
        "    x_filtered=x_filtered.drop(['y'], axis=1)\n",
        "    top_20_features= ['ID', 'X14', 'X29', 'X54', 'X76', 'X118', 'X119', 'X127', 'X132',\n",
        "       'X136', 'X189', 'X222', 'X232', 'X263', 'X279', 'X311', 'X314', 'X315',\n",
        "       'X6_g', 'X6_j']\n",
        "    def calc_vif(X):\n",
        "\n",
        "        # Calculationg VIF\n",
        "        vif=pd.DataFrame()\n",
        "        vif['variables']=X.columns\n",
        "        vif['VIF']= [variance_inflation_factor(X.values, i) for i in tqdm_notebook(range(X.shape[1]))]\n",
        "        \n",
        "        return (vif)\n",
        "    '''vif_values=calc_vif(x_filtered)\n",
        "    vif_values=vif_values.sort_values(by='VIF', ascending=True)\n",
        "    lst_variables= list(vif_values['variables'])\n",
        "    lst_vif= list(vif_values['VIF'])\n",
        "    vif_dict = dict(zip(lst_variables, lst_vif))'''\n",
        "    '''inf_indices=[]\n",
        "    count=0\n",
        "    for k,v in vif_dict.items():\n",
        "        if np.isinf(v):\n",
        "            inf_indices.append(k)\n",
        "            count+=1'''\n",
        "    '''# removing top_20_features from the features that need to be dropped from VIF\n",
        "    inf_indices=list(set(inf_indices)-set(top_20_features))'''\n",
        "    with open('/content/drive/My Drive/Colab Notebooks/Case Study 1/inf_indices_complete.txt', 'r') as f:\n",
        "        checc=json.loads(f.read())\n",
        "    inf_indices=checc\n",
        "    # dropping the features \n",
        "    x_filtered = x_filtered.drop(inf_indices, axis=1)\n",
        "    x_dummies_2= x_dummies_2.drop(inf_indices, axis=1)\n",
        "    # generating SVD features\n",
        "    tsvd= TruncatedSVD(n_components=2, random_state=42)\n",
        "    tsvd_train= tsvd.fit_transform(x_filtered)\n",
        "    tsvd_test=  tsvd.transform(x_dummies_2)\n",
        "    # generating PCA features\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    pca_train= pca.fit_transform(x_filtered)\n",
        "    pca_test= pca.transform(x_dummies_2)\n",
        "    # generating ICA features\n",
        "    ica=FastICA(n_components=2, random_state=42)\n",
        "    ica_train= ica.fit_transform(x_filtered)\n",
        "    ica_test= ica.transform(x_dummies_2)\n",
        "    # adding these dimensionality reduction features to the dataset\n",
        "    for i in range(0, tsvd_train.shape[1]):\n",
        "        x_filtered['tsvd_'+str(i)]= tsvd_train[:, i] \n",
        "        x_dummies_2['tsvd_'+str(i)]= tsvd_test[:, i]\n",
        "        x_filtered['pca_'+str(i)]= pca_train[:, i]\n",
        "        x_dummies_2['pca_'+str(i)]= pca_test[:, i]\n",
        "        x_filtered['ica_'+str(i)]= ica_train[:, i]\n",
        "        x_dummies_2['ica_'+str(i)]= ica_test[:, i]\n",
        "\n",
        "    # adding new features using feature interaction of important features\n",
        "    x_filtered['X64 + X218']=x_filtered['X64']+x_filtered['X218']\n",
        "    x_dummies_2['X64 + X218']=x_dummies_2['X64']+x_dummies_2['X218']\n",
        "    \n",
        "    x_filtered['X218 + X224 + X273']=x_filtered['X218']+x_filtered['X224'] + x_filtered['X273']\n",
        "    x_dummies_2['X218 + X224 + X273']=x_dummies_2['X218']+x_dummies_2['X224'] + x_dummies_2['X273']\n",
        "    \n",
        "    x_filtered['X64 + X224 + X273']=x_filtered['X64']+x_filtered['X224'] + x_filtered['X273']\n",
        "    x_dummies_2['X64 + X224 + X273']=x_dummies_2['X64']+x_dummies_2['X224'] + x_dummies_2['X273']\n",
        "    \n",
        "    x_filtered['X314 + X315']=x_filtered['X314']+x_filtered['X315']\n",
        "    x_dummies_2['X314 + X315']=x_dummies_2['X314']+x_dummies_2['X315']\n",
        "    \n",
        "    x_filtered['X314 + X315 + X29']=x_filtered['X314']+x_filtered['X315'] + x_filtered['X29']\n",
        "    x_dummies_2['X314 + X315 + X29']=x_dummies_2['X314']+x_dummies_2['X315'] + x_dummies_2['X29']\n",
        "    \n",
        "    x_filtered['X314 + X315 + X118']=x_filtered['X314']+x_filtered['X315'] + x_filtered['X118']\n",
        "    x_dummies_2['X314 + X315 + X118']=x_dummies_2['X314']+x_dummies_2['X315'] + x_dummies_2['X118']\n",
        "    \n",
        "    x_filtered['X127 + X189']=x_filtered['X127']+x_filtered['X189']\n",
        "    x_dummies_2['X127 + X189']=x_dummies_2['X127']+x_dummies_2['X189']\n",
        "    \n",
        "    x_filtered['X118 + X54']=x_filtered['X118']+x_filtered['X54']\n",
        "    x_dummies_2['X118 + X54']=x_dummies_2['X118']+x_dummies_2['X54']\n",
        "    \n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_filtered, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # applying RandomForestRegressor model \n",
        "    # Number of trees in a Random forest\n",
        "    n_estimators=[int(x) for x in np.linspace (start=100, stop=1000, num=10)]\n",
        "    \n",
        "    # Number of features to consider at every split\n",
        "    max_features=['auto', 'sqrt']\n",
        "     \n",
        "    # Maximun no of levels in a tree\n",
        "    max_depth=[int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "    max_depth.append(None)\n",
        "    \n",
        "    # minimum number of samples required to split a node\n",
        "    min_samples_split = np.arange(50,250,20)\n",
        "    \n",
        "    # Minimum number of samples required at each leaf node\n",
        "    min_samples_leaf = np.arange(5,50,5)\n",
        "    # Method of selecting samples for training each tree\n",
        "    bootstrap = [True, False]\n",
        "    \n",
        "    # Create the random grid\n",
        "    random_grid = {'n_estimators': n_estimators,\n",
        "                   'max_features': max_features,\n",
        "                   'max_depth': max_depth,\n",
        "                   'min_samples_split': min_samples_split,\n",
        "                   'min_samples_leaf': min_samples_leaf,\n",
        "                   'bootstrap': bootstrap}\n",
        "    # First create the base model to tune\n",
        "    rf = RandomForestRegressor()\n",
        "    # Random search of parameters, using 3 fold cross validation, \n",
        "    # search across 100 different combinations, and use all available cores\n",
        "    '''rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "    # Fit the random search model \n",
        "    rf_random.fit(x_filtered, y)\n",
        "    best_rf=rf_random.best_estimator_'''\n",
        "    best_rf= RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
        "                          max_depth=70, max_features='auto', max_leaf_nodes=None,\n",
        "                          max_samples=None, min_impurity_decrease=0.0,\n",
        "                          min_impurity_split=None, min_samples_leaf=40,\n",
        "                          min_samples_split=110, min_weight_fraction_leaf=0.0,\n",
        "                          n_estimators=500, n_jobs=None, oob_score=False,\n",
        "                          random_state=None, verbose=0, warm_start=False)\n",
        "    # fitting the best model on to the dataset\n",
        "    best_rf.fit(x_filtered, y)\n",
        "    y_train_pred=best_rf.predict(x_filtered)\n",
        "    y_test_pred=best_rf.predict(x_dummies_2)\n",
        "    \n",
        "    return r2_score(y, y_train_pred)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YD-19bib66O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b3510eb-f7e1-464e-8d35-a2426c0b44d3"
      },
      "source": [
        "final_metric(df_1, df_2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6535470835738704"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE_1u3NX9VxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_df= pd.DataFrame(list(zip(df_2['ID'].values, y_test_pred)), columns=['ID', 'y'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INw0WxuZ-ORD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "ba5afc0b-8f1a-4852-f9b5-9d66149fffc4"
      },
      "source": [
        "final_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>78.505489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>94.308220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>78.433665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>78.505489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>113.414816</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID           y\n",
              "0   1   78.505489\n",
              "1   2   94.308220\n",
              "2   3   78.433665\n",
              "3   4   78.505489\n",
              "4   5  113.414816"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPZUas4O-QdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#final_df.to_csv(r'/content/drive/My Drive/Colab Notebooks/Case Study 1/RandomForestRegressor_1.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}